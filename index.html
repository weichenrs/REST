<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  <!-- Meta tags for social media banners, these should be filled in appropriatly as they are your "business card" -->
  <!-- Replace the content tag with appropriate information -->
  <meta name="description" content="DESCRIPTION META TAG">
  <meta property="og:title" content="SOCIAL MEDIA TITLE TAG"/>
  <meta property="og:description" content="SOCIAL MEDIA DESCRIPTION TAG TAG"/>
  <meta property="og:url" content="URL OF THE WEBSITE"/>
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X630-->
  <meta property="og:image" content="fig/github-mark.png" />
  <meta property="og:image:width" content="1200"/>
  <meta property="og:image:height" content="630"/>


  <meta name="twitter:title" content="TWITTER BANNER TITLE META TAG">
  <meta name="twitter:description" content="TWITTER BANNER DESCRIPTION META TAG">
  <!-- Path to banner image, should be in the path listed below. Optimal dimenssions are 1200X600-->
  <meta name="twitter:image" content="static/images/your_twitter_banner_image.png">
  <meta name="twitter:card" content="summary_large_image">
  <!-- Keywords for your paper to be indexed by-->
  <meta name="keywords" content="KEYWORDS SHOULD BE PLACED HERE">
  <meta name="viewport" content="width=device-width, initial-scale=1">


  <title>REST</title>
<!--   <link rel="icon" type="image/x-icon" href="static/images/favicon.ico"> -->
  <link href="https://fonts.googleapis.com/css?family=Google+Sans|Noto+Sans|Castoro"
  rel="stylesheet">

  <link rel="stylesheet" href="static/css/bulma.min.css">
  <link rel="stylesheet" href="static/css/bulma-carousel.min.css">
  <link rel="stylesheet" href="static/css/bulma-slider.min.css">
  <link rel="stylesheet" href="static/css/fontawesome.all.min.css">
  <link rel="stylesheet"
  href="https://cdn.jsdelivr.net/gh/jpswalsh/academicons@1/css/academicons.min.css">
  <link rel="stylesheet" href="static/css/index.css">

  <script src="https://ajax.googleapis.com/ajax/libs/jquery/3.5.1/jquery.min.js"></script>
  <script src="https://documentcloud.adobe.com/view-sdk/main.js"></script>
  <script defer src="static/js/fontawesome.all.min.js"></script>
  <script src="static/js/bulma-carousel.min.js"></script>
  <script src="static/js/bulma-slider.min.js"></script>
  <script src="static/js/index.js"></script>
</head>
<body>


  <section class="hero">
    <div class="hero-body">
      <div class="container is-max-desktop">
        <div class="columns is-centered">
          <div class="column has-text-centered">
            <h1 class="title is-1 publication-title"> REST: Holistic Learning for End-to-End Semantic Segmentation of Whole-Scene Remote Sensing Imagery</h1>
            <div class="is-size-5 publication-authors">
              <!-- Paper authors -->
              <span class="author-block">
                <a href="https://scholar.google.com/citations?hl=en&user=vSGli2AAAAAJ">Wei Chen</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://webapps.unitn.it/du/en/Persona/PER0004714/Curriculum">Lorenzo Bruzzone</a><sup>2</sup>,</span>
              <span class="author-block">
                <a href="https://scholar.google.com/citations?user=AXlR4OwAAAAJ&hl=zh-CN">Bo Dang</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://yuan-gao.net">Yuan Gao</a><sup>1</sup>,</span>
              <span class="author-block">
                <a href="https://denghilbert.github.io">Youming Deng</a><sup>3</sup>,</span>
              <span class="author-block">
                <a href="https://yanzhao.scut.edu.cn/ExpertInfo.aspx?zjbh=goi4tp1KbAaC80CVL590Bg==">Jin-Gang Yu</a><sup>4</sup>,</span>
              <span class="author-block">
                <a href="https://liangqiy.com">Liangqi Yuan</a><sup>5</sup>,</span>
              <span class="author-block">
                <a href="https://jszy.whu.edu.cn/liyansheng/index.htm">Yansheng Li</a><sup>1</sup>
                  </span>
                  </div>

                  <div class="is-size-5 publication-authors">
                      <span class="author-block"><sup>1</sup>Wuhan University,</span>
                      <span class="author-block"><sup>2</sup>University of Trento</span>
                      <span class="author-block"><sup>3</sup>Cornell University</span>
                      <span class="author-block"><sup>4</sup>South China University of Technology</span>
                      <span class="author-block"><sup>5</sup>Purdue University<br>IEEE TPAMI 2025</span>
                  </div>

                  <div class="column has-text-centered">
                    <div class="publication-links">
                         <!-- Arxiv PDF link -->
                      <span class="link-block">
                        <a href="https://ieeexplore.ieee.org/document/11163637" target="_blank"
                        class="external-link button is-normal is-rounded is-dark">
                        <span class="icon">
                          <i class="fas fa-file-pdf"></i>
                        </span>
                        <span>Paper</span>
                      </a>
                    </span>

                    <!-- Supplementary PDF link -->
                    <span class="link-block">
                      <a href="static/pdfs/supplementary_material.pdf" target="_blank"
                      class="external-link button is-normal is-rounded is-dark">
                      <span class="icon">
                        <i class="fas fa-file-pdf"></i>
                      </span>
                      <span>Supplementary (Coming soon)</span>
                    </a>
                  </span>

                  <!-- Github link -->
                  <span class="link-block">
                    <a href="https://github.com/weichenrs/REST_code" target="_blank"
                    class="external-link button is-normal is-rounded is-dark">
                    <span class="icon">
                      <i class="fab fa-github"></i>
                    </span>
                    <span>Code</span>
                  </a>
                </span>
            </div>
          </div>
        </div>
      </div>
    </div>
  </div>
</section>


<!-- Teaser video-->
<section class="hero teaser">
  <div class="container is-max-desktop">
    <div class="hero-body">
      <video poster="" id="tree" autoplay controls muted loop height="100%">
        <!-- Your video here -->
        <source src="fig/REST.mp4"
        type="video/mp4">
      </video>
      <h2 class="subtitle has-text-centered">
        REST enables mainstream deep learning-based methods to support holistic segmentation of whole-scene remote sensing imagery. 
      </h2>
    </div>
  </div>
</section>
<!-- End teaser video -->

<!-- Paper abstract -->
<section class="section hero is-light">
  <div class="container is-max-desktop">
    <div class="columns is-centered has-text-centered">
      <div class="column is-four-fifths">
        <h2 class="title is-3">Abstract</h2>
        <div class="content has-text-justified">
          <p>
            Semantic segmentation of remote sensing imagery (RSI) is a fundamental task that aims at assigning a category label to each pixel. To pursue precise segmentation with one or more fine-grained categories, semantic segmentation often requires holistic segmentation of whole-scene RSI (WRI), which is normally characterized by a large size. However, conventional deep learning methods struggle to handle holistic segmentation of WRI due to the memory limitations of the graphics processing unit (GPU), thus requiring to adopt suboptimal strategies such as cropping or fusion, which result in performance degradation. Here, we introduce the Robust End-to-end semantic Segmentation architecture for whole-scene remoTe sensing imagery (REST). REST is the first intrinsically end-to-end framework for truly holistic segmentation of WRI, supporting a wide range of encoders and decoders in a plug-and-play fashion. It enables seamless integration with mainstream semantic segmentation methods, and even more advanced foundation models. Specifically, we propose a novel spatial parallel interaction mechanism (SPIM) within REST to overcome GPU memory constraints and achieve global context awareness. Unlike traditional parallel methods, SPIM enables REST to process a WRI effectively and efficiently by combining parallel computation with a divide-and-conquer strategy. Both theoretical analysis and experiments demonstrate that REST attains near-linear throughput scalability as additional GPUs are employed. Extensive experiments demonstrate that REST consistently outperforms existing cropping-based and fusion-based methods across a variety of scenarios, ranging from single-class to multi-class segmentation, from multispectral to hyperspectral imagery, and from satellite to drone platforms. The robustness and versatility of REST are expected to offer a promising solution for the holistic segmentation of WRI, with the potential for further extension to large-size medical imagery segmentation. The source code will be released at https://weichenrs.github.io/REST.
          </p>
        </div>
      </div>
    </div>
  </div>
</section>
<!-- End paper abstract -->


<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">
    
    <h2 class="title is-3 has-text-centered">The superiority of REST</h2>
    
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
        
       <div class="item">
         <h2 class="subtitle has-text-justified">
          Per-class result comparison on the Five-Billion-Pixels dataset and SkySense with UPerNet is chosen as the baseline.
        </h2>
        <!-- Your image here -->
        <img src="fig/fbp_swin+rest.png" alt="Per-class result comparison on the Five-Billion-Pixels dataset"/>
      </div>
        
      <div class="item">
          <h2 class="subtitle has-text-justified">
          Comparison of performance (IoU) and efficiency (inference time) across different methods on the GLH-Water dataset.
        </h2>
        <!-- Your image here -->
        <img src="fig/water_acc+time.png" alt="Comparison of performance (IoU) and efficiency (inference time)"/>
      </div>
        
      <div class="item">
        <h2 class="subtitle has-text-justified">
         Performance of REST with different baselines (i.e., encoders and decoders) on the Five-Billion-Pixels dataset.
       </h2>
        <!-- Your image here -->
        <img src="fig/fbp_all+rest.png" alt="Performance of REST with different baselines"/>
     </div>
        
     <div class="item">
       <h2 class="subtitle has-text-justified">
        Performance of REST on the Five-Billion-Pixels dataset with different image sizes, and the baseline is SkySense with UPerNet.
      </h2>
      <!-- Your image here -->
      <img src="fig/fbp_curve.png" alt="Performance of REST on the Five-Billion-Pixels dataset with different image sizes"/>
    </div>

    <div class="item">
      <h2 class="subtitle has-text-justified">
        REST further improves the strong capabilities of various remote sensing foundation models on the Five-Billion-Pixels dataset. RSFMs can only handle cropped image tiles with a size of 2048×2048. Integrating our REST, RSFMs can handle whole-scene remote sensing imagery with evidently improved performance and slightly increased model parameters. SkySense (Swin-Huge) requires 16 NVIDIA A100 GPUs for the holistic segmentation, while 4 NVIDIA A100 GPUs are enough for the others.
      </h2>
      <!-- Your image here -->
      <img src="fig/supfig_sizevspfm.png" alt="REST is compatible with remote sensing foundation models"/>
    </div>
        
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



<!-- Image carousel -->
<section class="hero is-small">
  <div class="hero-body">

    <h2 class="title is-3 has-text-centered">Explainability analysis of REST</h2>
    
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
              
      <div class="item">
        <h2 class="subtitle has-text-justified">
          Visualization of feature maps on Five-Billion-Pixels dataset. Enhanced with REST, the model can exploit the features of the entire spatial region in the WRI. SkySense with UPerNet is chosen as the baseline.
        </h2>
        <!-- Your image here -->
        <img src="fig/fig3_vis_fbp_attn.png" alt="Visualization of feature maps on Five-Billion-Pixels dataset"/>
      </div>
        
      <div class="item">
        <h2 class="subtitle has-text-justified">
         Visualization of t-SNE results on Five-Billion-Pixels dataset. SkySense with UPerNet is chosen as the baseline. When combined with REST, the features exhibit more distinct classification boundaries, demonstrating the strong feature representation ability brought by REST.
       </h2>
        <!-- Your image here -->
        <img src="fig/exfig4_vis_fbp_tsne.png" alt="Visualization of t-SNE results on Five-Billion-Pixels dataset"/>
     </div>
        
     <div class="item">
       <h2 class="subtitle has-text-justified">
        The confusion matrices of results on the Five-Billion-Pixels dataset. a, the confusion matrix of the baseline. b, the confusion matrix of baseline + our REST. SkySense with UPerNet is chosen as the baseline. After the introduction of REST, the accuracy performance across various categories improves, and the confusion between fine-grained categories decreases.
      </h2>
      <!-- Your image here -->
      <img src="fig/exfig5_conf_mat.png" alt="The confusion matrices of results on the Five-Billion-Pixels dataset"/>
    </div>
        
  </div>
</div>
</div>
</section>
<!-- End image carousel -->



  
<!-- Image carousel -->
<section class="hero is-small">

  <h2 class="title is-3 has-text-centered">Visualization of experimental results</h2>
  
  <div class="hero-body">
    <div class="container">
      <div id="results-carousel" class="carousel results-carousel">
       <div class="item">
         <h2 class="subtitle has-text-justified">
          Visualization of segmentation results on Five-Billion-Pixels dataset. REST successfully distinguishes between fine-grained categories (e.g., river, lake, pond), while the results of other competing methods mostly show confusion.
        </h2>
        <!-- Your image here -->
        <img src="fig/fig2_vis_fbp2.png" alt="Visualization of segmentation results on Five-Billion-Pixels dataset"/>
      </div>
        
      <div class="item">
        <h2 class="subtitle has-text-justified">
          Visualization of segmentation results on GLH-Water dataset. REST accurately extracts the complete water body, while the competing methods present omission errors in different locations. 
        </h2>
        <!-- Your image here -->
        <img src="fig/exfig1_vis_water.png" alt="Visualization of segmentation results on GLH-Water dataset"/>
      </div>
        
      <div class="item">
        <h2 class="subtitle has-text-justified">
         Visualization of segmentation results on UAVid dataset. Compared with other methods, REST precisely identifies the vehicle in the image as a moving car instead of a static car, completely extracts the road, and significantly reduces misclassification problems.
       </h2>
        <!-- Your image here -->
        <img src="fig/exfig3_vis_uavid.png" alt="Visualization of segmentation results on UAVid dataset"/>
     </div>
        
     <div class="item">
      <h2 class="subtitle has-text-justified">
        Visualization of segmentation results on WHU-OHS dataset. REST demonstrates better segmentation results than the competing methods even on the challenging hyperspectral imagery datasets.
      </h2>
      <!-- Your image here -->
      <img src="fig/exfig2_vis_whuohs.png" alt="Visualization of segmentation results on WHU-OHS dataset."/>
    </div>
        
  </div>
</div>
</div>
</section>
<!-- End image carousel -->

  
<!-- Single image -->
<section class="hero is-small">

  <h2 class="title is-3 has-text-centered">REST shows potential on large-size medical imagery segmentation</h2>
  
  <div class="hero-body">
    <div class="container">
      <div class="item">
        <h2 class="subtitle has-text-justified">
          Visualization of preliminary results on the ISIC dataset, a medical image dataset for skin lesion analysis. Using REST significantly enhances the extraction effectiveness of the lesion areas.
        </h2>
        <!-- Your image here -->
        <img src="fig/supfig_vis_isic.png" alt="Visualization of preliminary results on the ISIC dataset"/>
      </div>
    </div>
  </div>
</section>
<!-- End Single image -->



<!--BibTex citation -->
  <section class="section" id="BibTeX">
    <div class="container is-max-desktop content">
      <h2 class="title">BibTeX</h2>
      <pre><code>BibTex Code Is Coming Soon！</code></pre>
    </div>
</section>
<!--End BibTex citation -->



  <footer class="footer">
  <div class="container">
    <div class="columns is-centered">
      <div class="column is-8">
        <div class="content">

          <p>
            This page was built using the <a href="https://github.com/eliahuhorwitz/Academic-project-page-template" target="_blank">Academic Project Page Template</a> which was adopted from the <a href="https://nerfies.github.io" target="_blank">Nerfies</a> project page.
            You are free to borrow the source code of this website, we just ask that you link back to this page in the footer. <br> This website is licensed under a <a rel="license"  href="http://creativecommons.org/licenses/by-sa/4.0/" target="_blank">Creative
            Commons Attribution-ShareAlike 4.0 International License</a>.
          </p>

        </div>
      </div>
    </div>
  </div>
</footer>

<!-- Statcounter tracking code -->
  
<!-- You can add a tracker to track page visits by creating an account at statcounter.com -->

    <!-- End of Statcounter Code -->

  </body>
  </html>
